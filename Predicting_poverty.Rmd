---
title: "Predicting_poverty"
author: "Farooq Qaiser"
date: "February 14, 2018"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Admin

## set seed

```{r}

## set the seed to make rmd reproducible
set.seed(123)

```


## load libraries 

```{r}

# don't want this mask margin from ggplot2
library(randomForest)

library(dplyr)
library(tidyr)
library(ggplot2)

library(dummies)
library(keras)

```


## load data

```{r}

data_loc = "/home/fqaiser94/poverty_data"

```

set data location

### household data

```{r}


load_data <- function(data_loc, file_pattern) {
  
  # find files matching pattern
  files = list.files(path = data_loc, pattern = file_pattern)	
  file_paths = paste0(data_loc, '/', files)
  
  # init empty list
  data_list <- list()
  
  # for each file matching file pattern
  for(i in 1:length(file_paths)) {
    
    file = file_paths[i]
    
    print(file)
    
    # read in file as df
    raw_data = read.csv(
      file = file,
      header = TRUE,
      stringsAsFactors = FALSE
      )
    
    # save df to list
    data_list[[i]] <- raw_data
  }
  
  # return list of data
  return(data_list)
}

```

define generic load data function

```{r}

country = list()
country[[1]] <- 'A'
country[[2]] <- 'B'
country[[3]] <- 'C'

raw_train_hhold_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*hhold.*train.*')

lapply(
  X = raw_train_hhold_data, 
  FUN = head
  )


```

get training household level files

```{r}

raw_test_hhold_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*hhold.*test.*')

lapply(
  X = raw_test_hhold_data, 
  FUN = head
  )

```

get test household level files

### individual data

```{r}

raw_train_indiv_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*indiv.*train.*')

lapply(
  X = raw_train_indiv_data, 
  FUN = head
  )

```

get training individual level files

```{r}

raw_test_indiv_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*indiv.*test.*')

lapply(
  X = raw_test_indiv_data, 
  FUN = head
  )


```

get test individual level files

# EDA

## column names

```{r}

lapply(
  X = raw_train_hhold_data, 
  FUN = colnames
  )

```

The majority of the columns' names have been masked.  
id, country, poor are the only ones I see which haven't been masked.  
Worse yet, columns are not aligned across the 3 household datasets. 

## missing data

```{r}

ggplot_missing <- function(x){
  
  x %>% 
    is.na %>%
    reshape2::melt() %>%
    ggplot(
      aes(x = Var2, y = Var1)) +
    geom_raster(
      aes(fill = value)) +
    scale_fill_grey(
      name = "",
      labels = c("Present","Missing")) +
    theme_minimal() + 
    theme(
      axis.ticks = element_line(), 
      axis.text.x  = element_blank()) + 
    labs(
      x = "Variables",
      y = "Rows")
}

# lapply(
#   X = raw_train_hhold_data,
#   FUN = ggplot_missing
#   )

```

```{r}

# count number of NA
temp_function <- function(df){
  
  sum(is.na(df)) 
  
  }

lapply(
  X = raw_train_hhold_data,
  FUN = temp_function
  )

```

only the second dataset has any missing data. 

## class balance

```{r}

temp_function <- function(df) {
  
  df %>%
    ggplot(mapping = aes(x = poor)) +
    geom_bar() + 
    theme_minimal()
  
}

lapply(
  X = raw_train_hhold_data, 
  FUN = temp_function
  )

```

only one is a balanced dataset

# Preprocess data

```{r}

# model_train_hhold_data = raw_train_hhold_data

```

create new df for preprocessed data

```{r}

model_train_hhold_data = list()

for (i in seq_along(raw_train_hhold_data)) {

  model_train_hhold_data[[i]] <- raw_train_hhold_data[[i]] %>%
    mutate(Train=1) %>%
    bind_rows(
      raw_test_hhold_data[[i]] %>% 
        mutate(Train=0)) %>%
    mutate(poor = ifelse(is.na(poor), 'False', poor))

}

```

combined train and test data temporarily so we transform them in exactly the same way

```{r}

# lapply(
#   X = model_train_hhold_data,
#   FUN = ggplot_missing
#   )

```

where the poor column is coming from the testing data, it is missing across all three datasets.  

## treat NULL observations

```{r}

for (i in seq_along(model_train_hhold_data)) {
  
  # for dropping rows with NA
  # model_train_hhold_data[[i]] <-  
  #   model_train_hhold_data[[i]][complete.cases(, model_train_hhold_data[[i]])]
  
  # for dropping columns with NA
  model_train_hhold_data[[i]] <- 
    model_train_hhold_data[[i]][ , colSums(
      is.na(model_train_hhold_data[[i]]))==0]

}

```

```{r}

# count number of NA
temp_function <- function(df){
  
  sum(is.na(df)) 
  
  }

lapply(
  X = model_train_hhold_data,
  FUN = temp_function
  )

```

remove any columns with NULL values

## correct data types

```{r}

temp_function <- function(df) {
  
  unique(df$poor)
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

Our target variable appears to be encoded as strings. Let's double check that. 

```{r}

temp_function <- function(df) {
  
  sapply(select(df, poor), class)
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

yup, character it is.   

```{r}

for (i in seq_along(model_train_hhold_data)) {
  model_train_hhold_data[[i]]$poor <- ifelse(
    model_train_hhold_data[[i]]$poor=="True", 2, 1)
  
  print(class(model_train_hhold_data[[i]]$poor))

}


```

changed to numeric

```{r}

temp_function <- function(df) {
  
  unique(sapply(df, class))
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

got four types of features in each dataset

```{r}

filter_by_column_type = function(df, type) {
  
  ind <- sapply(df, type)
  
  return(df[ind])
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = filter_by_column_type, 
  type = is.numeric # integer or double (numeric)
  )

```

relatively few numeric (integer or double) features

```{r}

lapply(
  X = model_train_hhold_data, 
  FUN = filter_by_column_type, 
  type = is.character
  )

```

tons of character columns, relatively speaking.  

## standardize numeric


```{r}

# standardize numeric features
standardize <- function(df, omit_columns) {
 
  # identify columns for standardization
  ind = sapply(df, is.numeric)
  
  # omit specific columns for standardization
  ind[which(colnames(df) %in% omit_columns)] <- FALSE

  # standardize columns
  df[, ind] = lapply(df[, ind], scale)
  df[, ind] = lapply(df[, ind], as.vector)
  
  return(df)
}

model_train_hhold_data <- lapply(
  X = model_train_hhold_data, 
  FUN = standardize, 
  omit_columns = c('id', 'Train', 'poor')
  )

```


```{r}

lapply(
  X = model_train_hhold_data, 
  FUN = filter_by_column_type, 
  type = is.numeric # integer or double (numeric)
  )

```

standardize numeric columns (excluding the id and train columns)

## OHE categorical

```{r}

# OHE categorical features
model_train_hhold_data <- lapply(
  X = model_train_hhold_data, 
  FUN = dummy.data.frame, 
  dummy.classes = c('character'),
  sep = "_"
  )

```

one hot encode character columns

```{r}

head(model_train_hhold_data[[1]])

```

looks about right

## split train validation test


```{r}

test = list()

for (i in seq_along(model_train_hhold_data)) {
  
  test[[i]] <- model_train_hhold_data[[i]] %>%
    filter(Train==0) %>%
    select(-Train
           #, -poor
           )
  
  model_train_hhold_data[[i]] <- model_train_hhold_data[[i]] %>%
    filter(Train==1) %>%
    select(-Train)

}

```

extract testing data

```{r}

train_inds = list()

train = list()
valid = list()

for (i in seq_along(model_train_hhold_data)) {
  
  # sample indices for training data
  sample_size <- floor(0.75 * nrow(model_train_hhold_data[[i]]))
  
  # save to list, we'll need these later to get back the id column
  train_inds[[i]] <- sample(
    seq_len(nrow(model_train_hhold_data[[i]])), size = sample_size)
  
  train[[i]] <- subset(
    x =model_train_hhold_data[[i]][train_inds[[i]], ], select = -c(id))
  
  valid[[i]] <- subset(
    model_train_hhold_data[[i]][-train_inds[[i]], ], select = -c(id))

}

```

split into train and valid samples

## treat class imbalance in training data

```{r}

get_class_weights <- function(target_var) {
  
  # y: target variable
  
  # calculate total number of responses
  total = length(target_var)
  
  # init empty dictionary
  dict = data.frame(class=NULL, count=NULL, stringsAsFactors = FALSE)
  
  for(class in unique(target_var)) {
    
    dict <- dict %>%
      bind_rows(
        data.frame(
          class = class, 
          count = sum(target_var==class), 
          stringsAsFactors = FALSE
        )
      )
  }
  
  # figure out major class
  majority_count = max(dict$count)
  majority_class = dict[dict$count==majority_count, ]$class

  # create weights 
  dict$weight = majority_count/dict$count

  return(dict)
  
  } 

class_weights = list()

for (i in seq_along(model_train_hhold_data)) {

  
  class_weights[[i]] =  get_class_weights(
    target_var = train[[i]]$poor)
  
  print(class_weights[[i]])
  
}

```


```{r}


get_class_weights <- function(target_var) {
  
  # y: target variable
  
  # calculate total number of responses
  total = length(target_var)
  
  # init empty dictionary
  counts_dict = list()
  
  for(class in unique(target_var)) {
    
    counts_dict[[class]] <- sum(target_var==class)
  }
  
  # figure out major class
  majority_class = which.max(counts_dict)
  majority_count = as.numeric(counts_dict[majority_class]) 

  # init empty dictionary
  weights_dict = list()
  
  for(class in unique(target_var)) {
    
    weights_dict[[class]] <- majority_count/as.numeric(counts_dict[class])
  
    }
  
  return(weights_dict)
  
  } 

class_weights = list()

for (i in seq_along(model_train_hhold_data)) {


  class_weights[[i]] =  get_class_weights(
    target_var = train[[i]]$poor)

}

```


# Modelling

## logistic regression

insert code here

## random forests model


```{r}

# rf = randomForest(
#   factor(poor) ~ . , 
#   data = train 
#   )
# 
# rf

```

```{r}

# plot(rf)

```

```{r}

# varImpPlot(rf)

```

## lightGBM

useful resources:  
- https://www.kaggle.com/andrewmvd/lightgbm-in-r  
- https://github.com/Microsoft/LightGBM/tree/master/R-package  

## deep learning

recommended resources for deep learning in keras:  
- https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X  
- https://github.com/rstudio/cheatsheets/raw/master/keras.pdf  
- https://www.datacamp.com/community/tutorials/keras-r-deep-learning  
- https://www.linkedin.com/pulse/finally-deep-learning-keras-tensorflow-r-richard-wanjohi-ph-d/  
- https://cran.rstudio.com/web/packages/keras/vignettes/about_keras_models.html 


```{r}

# remove target variable and convert to data matrix
temp_function <- function(df) {
  
  feature_data <- data.matrix(select(df, -poor))

}

x_train <- lapply(
  X = train, 
  FUN = temp_function 
  )

x_valid <- lapply(
  X = valid, 
  FUN = temp_function
  )

x_test <- lapply(
  X = test, 
  FUN = temp_function
  )

```


```{r}

# extract target variable and convert to data matrix
temp_function <- function(df) {
  
  y = to_categorical(df$poor)

  }

y_train <- lapply(
  X = train, 
  FUN = temp_function 
  )

y_valid <- lapply(
  X = valid,
  FUN = temp_function 
  )

```


```{r}

for (i in seq_along(train)) {
  
  print(ncol(x_train[[i]]))

}

```


```{r}

# # init empty list of models
# models = list()
# 
# for (i in seq_along(model_train_hhold_data)) {
# 
#   models[[i]] <- keras_model_sequential() %>% 
#     layer_dense(units = 3000, activation = 'relu', 
#       input_shape = c(ncol(x_train[[i]]))) %>% 
#     layer_dropout(rate = 0.4) %>%
#     layer_dense(units = 1000, activation = 'relu') %>%
#     layer_dropout(rate = 0.3) %>%
#     layer_dense(units = 512, activation = 'relu') %>%
#     layer_dropout(rate = 0.2) %>%
#     layer_dense(units = 256, activation = 'relu') %>%
#     layer_dropout(rate = 0.1) %>%
#     layer_dense(units = 50, activation = 'relu') %>%
#     layer_dropout(rate = 0.05) %>%
#     # softmax guarantees output between 0 and 1; pseudo probability 
#     layer_dense(units = 2, activation = 'softmax') %>% 
#     # 
#     compile(
#       # this is the metric used for the competition
#       loss = 'binary_crossentropy', 
#       # 
#       optimizer = optimizer_rmsprop(),
#       # 
#       metrics = c('accuracy')
#     )
# }

```

```{r}

# init empty list of models
models = list()

models[[1]] <- keras_model_sequential() %>% 
  layer_dense(units = 3000, activation = 'relu', 
    input_shape = c(ncol(x_train[[1]]))) %>% 
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1000, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 512, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.05) %>%
  # softmax guarantees output between 0 and 1; pseudo probability 
  layer_dense(units = 2, activation = 'softmax') %>% 
  # 
  compile(
    # this is the metric used for the competition
    loss = 'binary_crossentropy', 
    # 
    optimizer = optimizer_adadelta(),
    # 
    metrics = c('accuracy')
  )

models[[2]] <- keras_model_sequential() %>% 
  layer_dense(units = 5000, activation = 'relu', 
    input_shape = c(ncol(x_train[[2]]))) %>% 
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 3000, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1000, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 512, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.05) %>%
  # softmax guarantees output between 0 and 1; pseudo probability 
  layer_dense(units = 2, activation = 'softmax') %>% 
  # 
  compile(
    # this is the metric used for the competition
    loss = 'binary_crossentropy', 
    # 
    optimizer = optimizer_adadelta(),
    # 
    metrics = c('accuracy')
  )

models[[3]] <- keras_model_sequential() %>% 
  layer_dense(units = 400, activation = 'relu', 
    input_shape = c(ncol(x_train[[3]]))) %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 200, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.05) %>%
  # softmax guarantees output between 0 and 1; pseudo probability 
  layer_dense(units = 2, activation = 'softmax') %>% 
  # 
  compile(
    # this is the metric used for the competition
    loss = 'binary_crossentropy', 
    # 
    optimizer = optimizer_adadelta(),
    # 
    metrics = c('accuracy')
  )

```


```{r}

```


```{r}

# models[[2]] <- keras_model_sequential() %>% 
#   layer_dense(units = 1000, activation = 'relu', 
#     input_shape = c(ncol(x_train[[2]]))) %>% 
#   layer_dropout(rate = 0.5) %>%
#   layer_dense(units = 500, activation = 'relu') %>%
#   layer_dropout(rate = 0.4) %>%
#   layer_dense(units = 250, activation = 'relu') %>%
#   layer_dropout(rate = 0.3) %>%
#   layer_dense(units = 100, activation = 'relu') %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 50, activation = 'relu') %>%
#   layer_dropout(rate = 0.1) %>%
#   layer_dense(units = 10, activation = 'relu') %>%
#   layer_dropout(rate = 0) %>%
#   # softmax guarantees output between 0 and 1; pseudo probability 
#   layer_dense(units = 2, activation = 'softmax') %>% 
#   # 
#   compile(
#     # this is the metric used for the competition
#     loss = 'binary_crossentropy', 
#     # 
#     optimizer = optimizer_adadelta(),
#     # 
#     metrics = c('accuracy')
#   )
# 
# history <- models[[2]] %>% 
#   fit(
#     x = x_train[[2]], 
#     y = y_train[[2]], 
#     epochs = 10, #5,
#     batch_size = 128, 
#     validation_split = 0.2)

```


```{r}

# i = 3
# 
# print(ncol(x_train[[i]]))
# print(nrow(x_train[[i]]))
# 
# models[[i]] <- keras_model_sequential() %>% 
#   layer_dense(units = 400, activation = 'relu', 
#     input_shape = c(ncol(x_train[[3]]))) %>% 
#   layer_dropout(rate = 0.3) %>%
#   layer_dense(units = 200, activation = 'relu') %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 100, activation = 'relu') %>%
#   layer_dropout(rate = 0.1) %>%
#   layer_dense(units = 50, activation = 'relu') %>%
#   layer_dropout(rate = 0.05) %>%
#   # softmax guarantees output between 0 and 1; pseudo probability 
#   layer_dense(units = 2, activation = 'softmax') %>% 
#   # 
#   compile(
#     # this is the metric used for the competition
#     loss = 'binary_crossentropy', 
#     # 
#     optimizer = optimizer_rmsprop(),
#     # 
#     metrics = c('accuracy')
#   )
# 
# history <- models[[i]] %>% 
#   fit(
#     x = x_train[[i]], 
#     y = y_train[[i]], 
#     epochs = 10, #5,
#     batch_size = 128, 
#     validation_split = 0.2)


```


arbitrary deep learning architecture  

consider using a multiscale architecture  

Other loss functions available here: https://keras.io/losses/  

```{r}

history = list() 

for (i in seq_along(models)) {

  history[[i]] <- models[[i]] %>% 
    fit(
      x = x_train[[i]], 
      y = y_train[[i]],
      class_weight=class_weights[[i]],
      epochs = 5, #5,
      batch_size = 128, 
      validation_split = 0.2, 
      shuffle = TRUE) 
}

```


```{r}

for (i in seq_along(models)) {
  
  print(plot(history[[i]]))
  
}

```


```{r}

for (i in seq_along(models)) {
  
  temp <- models[[i]] %>% 
    evaluate(x_valid[[i]], y_valid[[i]])
  
  print(temp)
}


```


```{r}

for (i in seq_along(models)) {
  
  # Plot the model loss of the training data
  plot(
    history[[i]]$metrics$loss, 
    main="Model Loss", 
    xlab = "epoch", 
    ylab="loss", 
    col="blue", 
    type="l")
  
  # Plot the model loss of the test data
  lines(
    history[[i]]$metrics$val_loss, 
    col="green")
  
  # Add legend
  legend(
    "topright", 
    c("train","test"), 
    col=c("blue", "green"), 
    lty=c(1,1))
}



```


```{r}

for (i in seq_along(models)) {
  
  # Plot the model accuracy
  plot(
    history[[i]]$metrics$acc, 
    main="Model Accuracy", 
    xlab = "epoch", 
    ylab="accuracy", 
    col="blue", 
    type="l")
  
  lines(
    history[[i]]$metrics$val_acc, 
    col="green")
  
  legend(
    "bottomright", 
    c("train","test"), 
    col=c("blue", "green"), 
    lty=c(1,1))
  
}

```

### validation predictions

```{r}

# test_predictions = list()
# 
# for (i in seq_along(models)) {
# 
#   test_predictions[[i]] <- models[[i]] %>%
#     # make predictions
#     predict_proba(x=x_test[[i]]) %>%
#     # convert to df
#     data.frame() %>%
#     rename(not_poor=X1, poor=X2) %>%
#     # add back country
#     mutate(country=country[[i]]) %>%
#     # add back id
#     bind_cols(
#       subset(model_train_hhold_data[[i]][-train_inds[[i]], ], select=c(id))) %>%
#     # we only want the probability that p = 1 (not p = 0)
#     select(id, country, poor)
#   }
# 
# head(test_predictions)

```

### test predictions


```{r}

test_predictions = list()

for (i in seq_along(models)) {

  test_predictions[[i]] <- models[[i]] %>%
    # make predictions
    predict_proba(x = x_test[[i]][, -1]) %>%
    # convert to df
    data.frame() %>%
    rename(not_poor=X1, poor=X2) %>%
    # add back country
    mutate(country=country[[i]]) %>%
    # add back id
    bind_cols(data.frame(id = x_test[[i]][, 1])) %>%
    # we only want the probability that p = 1 (not p = 0)
    select(id, country, poor)
  }

head(test_predictions)

```

got predictions 

```{r}

temp = do.call("rbind", test_predictions)

write.csv(x = temp, file = '/home/fqaiser94/Desktop/temp.csv', row.names = FALSE)

```

combine and save to csv
### interpreting the model

useful resources:  
- http://www.f1-predictor.com/model-interpretability-with-shap/?utm_source=linkedin&utm_medium=post&utm_campaign=shap 
- 

insert SHAP/LIME code

## ensemble

combine multiple predictors  

useful resources:  
- https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/  


### correlation check 

check models are relatively uncorrelated  

### average

```{r}

# #Predicting the probabilities
# testSet$pred_rf_prob <- predict(
#   object = model_rf, 
#   testSet[,predictors], 
#   type='prob')
# 
# testSet$pred_knn_prob <- predict(
#   object = model_knn, 
#   testSet[,predictors],
#   type='prob')
# 
# testSet$pred_lr_prob <- predict(
#   object = model_lr, 
#   testSet[,predictors],
#   type='prob')
# 
# # taking average of predictions
# testSet$pred_avg <-(
#   testSet$pred_rf_prob$Y + testSet$pred_knn_prob$Y + testSet$pred_lr_prob$Y)/3
# 
# # splitting into binary classes at 0.5
# testSet$pred_avg<-as.factor(
#   ifelse(
#     testSet$pred_avg>0.5,
#     'Y',
#     'N'))

```

### weighted average

```{r}

# # Taking weighted average of predictions
# testSet$pred_weighted_avg <- (testSet$pred_rf_prob$Y*0.25) + 
#   (testSet$pred_knn_prob$Y*0.25) +
#   (testSet$pred_lr_prob$Y*0.5)
# 
# #Splitting into binary classes at 0.5
# testSet$pred_weighted_avg <- as.factor(
#   ifelse(
#     testSet$pred_weighted_avg>0.5, 
#     'Y',
#     'N'))

```

### stacking  

