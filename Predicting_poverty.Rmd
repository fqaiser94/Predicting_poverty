---
title: "Predicting_poverty"
author: "Farooq Qaiser"
date: "February 14, 2018"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Admin

## set seed

```{r}

## set the seed to make rmd reproducible
set.seed(123)

```


## load libraries 

```{r}

# don't want this mask margin from ggplot2
library(randomForest)

library(dplyr)
library(tidyr)
library(ggplot2)

library(dummies)
library(keras)

```


## load data

```{r}

data_loc = "/home/fqaiser94/poverty_data"

```

set data location

### household data

```{r}


load_data <- function(data_loc, file_pattern) {
  
  # find files matching pattern
  files = list.files(path = data_loc, pattern = file_pattern)	
  file_paths = paste0(data_loc, '/', files)
  
  # init empty list
  data_list <- list()
  
  # for each file matching file pattern
  for(i in 1:length(file_paths)) {
    
    file = file_paths[i]
    
    print(file)
    
    # read in file as df
    raw_data = read.csv(
      file = file,
      header = TRUE,
      stringsAsFactors = FALSE
      )
    
    # save df to list
    data_list[[i]] <- raw_data
  }
  
  # return list of data
  return(data_list)
}

```

define generic load data function

```{r}

country = list()
country[[1]] <- 'A'
country[[2]] <- 'B'
country[[3]] <- 'C'

raw_train_hhold_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*hhold.*train.*')

lapply(
  X = raw_train_hhold_data, 
  FUN = head
  )


```

get training household level files

```{r}

raw_test_hhold_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*hhold.*test.*')

lapply(
  X = raw_test_hhold_data, 
  FUN = head
  )

```

get test household level files

### individual data

```{r}

raw_train_indiv_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*indiv.*train.*')

lapply(
  X = raw_train_indiv_data, 
  FUN = head
  )

```

get training individual level files

```{r}

raw_test_indiv_data <- load_data(
  data_loc = data_loc, 
  file_pattern = '.*indiv.*test.*')

lapply(
  X = raw_test_indiv_data, 
  FUN = head
  )


```

get test individual level files

# EDA

## column names

```{r}

lapply(
  X = raw_train_hhold_data, 
  FUN = colnames
  )

```

The majority of the columns' names have been masked.  
id, country, poor are the only ones I see which haven't been masked.  
Worse yet, columns are not aligned across the 3 household datasets. 

## missing data

```{r}

ggplot_missing <- function(x){
  
  x %>% 
    is.na %>%
    reshape2::melt() %>%
    ggplot(
      aes(x = Var2, y = Var1)) +
    geom_raster(
      aes(fill = value)) +
    scale_fill_grey(
      name = "",
      labels = c("Present","Missing")) +
    theme_minimal() + 
    theme(
      axis.ticks = element_line(), 
      axis.text.x  = element_blank()) + 
    labs(
      x = "Variables",
      y = "Rows")
}

# lapply(
#   X = raw_train_hhold_data,
#   FUN = ggplot_missing
#   )

```

```{r}

# count number of NA
temp_function <- function(df){
  
  sum(is.na(df)) 
  
  }

lapply(
  X = raw_train_hhold_data,
  FUN = temp_function
  )

```

only the second dataset has any missing data. 

## class balance

```{r}

temp_function <- function(df) {
  
  df %>%
    ggplot(mapping = aes(x = poor)) +
    geom_bar() + 
    theme_minimal()
  
}

lapply(
  X = raw_train_hhold_data, 
  FUN = temp_function
  )

```

only one is a balanced dataset

# Preprocess data

```{r}

model_train_hhold_data = raw_train_hhold_data

```

create new df for preprocessed data

## treat NULL observations

```{r}

for (i in seq_along(model_train_hhold_data)) {
  
  # for dropping rows with NA
  # model_train_hhold_data[[i]] <-  
  #   model_train_hhold_data[[i]][complete.cases(, model_train_hhold_data[[i]])]
  
  # for dropping columns with NA
  # inds <- (colSums(is.na(model_train_hhold_data[[i]]))) > 0 
  # 
  # print(sum(inds))
  # print(inds)
  # 
  # model_train_hhold_data[[i]] <- model_train_hhold_data[[i]][, -inds]
  
  model_train_hhold_data[[i]] <- 
    model_train_hhold_data[[i]][ , colSums(
      is.na(model_train_hhold_data[[i]]))==0]

}

```

```{r}

# count number of NA
temp_function <- function(df){
  
  sum(is.na(df)) 
  
  }

lapply(
  X = model_train_hhold_data,
  FUN = temp_function
  )

```

remove any columns with NULL values

```{r}

temp_function <- function(df) {
  
  unique(df$poor)
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

## correct data types

Our target variable appears to be encoded as strings. Let's double check that. 

```{r}

temp_function <- function(df) {
  
  sapply(select(df, poor), class)
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

yup, character.  

```{r}

for (i in seq_along(model_train_hhold_data)) {
  model_train_hhold_data[[i]]$poor <- ifelse(
    model_train_hhold_data[[i]]$poor=="True", TRUE, FALSE)
  
  print(class(model_train_hhold_data[[i]]$poor))

}


```

changed to boolean

```{r}

# for (i in seq_along(model_train_hhold_data)) {
#   
#   print(which(model_train_hhold_data[[i]]=="True", arr.ind = TRUE))
#   print(which(model_train_hhold_data[[i]]=="False", arr.ind = TRUE))
# 
# }

```

double checked and verified that there are no other columns where True/False is encoded as strings

```{r}

temp_function <- function(df) {
  
  unique(sapply(df, class))
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = temp_function
  )

```

got four types of features in each dataset

```{r}

filter_by_column_type = function(df, type) {
  
  ind <- sapply(df, type)
  
  return(df[ind])
  
}

lapply(
  X = model_train_hhold_data, 
  FUN = filter_by_column_type, 
  type = is.numeric # integer or double (numeric)
  )

```

only 4 numeric (integer or double) columns (excluding id column)

```{r}

lapply(
  X = model_train_hhold_data, 
  FUN = filter_by_column_type, 
  type = is.character
  )

```

tons of character columns

## standardize numeric

```{r}

# standardize numeric features
standardize <- function(df, omit_columns) {
 
  # identify columns for standardization
  ind = sapply(df, is.numeric)
  
  # omit specific columns for standardization
  ind[which(colnames(df) %in% omit_columns)] <- FALSE

  # standardize columns
  df[ind] = lapply(df[ind], scale)
  
  return(df)
}

model_train_hhold_data <- lapply(
  X = model_train_hhold_data, 
  FUN = standardize, 
  omit_columns = 'id'
  )

```

standardize numeric columns (excluding the id column)

```{r}

# OHE categorical features
model_train_hhold_data <- lapply(
  X = model_train_hhold_data, 
  FUN = dummy.data.frame, 
  dummy.classes = c('character'),
  sep = "_"
  )

```

## OHE categorical

one hot encode character columns

```{r}

head(model_train_hhold_data[[1]])

```

looks right

## split train validation

```{r}

train_inds = list()

train = list()
test = list()

for (i in seq_along(model_train_hhold_data)) {
  
  # sample indices for training data
  sample_size <- floor(0.75 * nrow(model_train_hhold_data[[i]]))
  
  # save to list, we'll need these later to get back the id column
  train_inds[[i]] <- sample(
    seq_len(nrow(model_train_hhold_data[[i]])), size = sample_size)
  
  train[[i]] <- subset(
    x =model_train_hhold_data[[i]][train_inds[[i]], ], select = -c(id))
  
  test[[i]] <- subset(
    model_train_hhold_data[[i]][-train_inds[[i]], ], select = -c(id))

}

```

split into train and test samples

# Modelling

## logistic regression

insert code here

## random forests model


```{r}

# rf = randomForest(
#   factor(poor) ~ . , 
#   data = train 
#   )
# 
# rf

```

```{r}

# plot(rf)

```

```{r}

# varImpPlot(rf)

```

## lightGBM

useful resources:  
- https://www.kaggle.com/andrewmvd/lightgbm-in-r  
- https://github.com/Microsoft/LightGBM/tree/master/R-package  

## deep learning

recommended resources for deep learning in keras:  
- https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X  
- https://github.com/rstudio/cheatsheets/raw/master/keras.pdf  
- https://www.datacamp.com/community/tutorials/keras-r-deep-learning  
- https://www.linkedin.com/pulse/finally-deep-learning-keras-tensorflow-r-richard-wanjohi-ph-d/  
- https://cran.rstudio.com/web/packages/keras/vignettes/about_keras_models.html 


```{r}

# remove target variable and convert to data matrix
temp_function <- function(df) {
  
  feature_data <- data.matrix(select(df, -poor))

}

x_train <- lapply(
  X = train, 
  FUN = temp_function 
  )

x_test <- lapply(
  X = test, 
  FUN = temp_function
  )

```


```{r}

# extract target variable and convert to data matrix
temp_function <- function(df) {
  
  y = to_categorical(df$poor)

  }

y_train <- lapply(
  X = train, 
  FUN = temp_function 
  )

y_test <- lapply(
  X = test,
  FUN = temp_function 
  )

```


```{r}

# init empty list of models
models = list()

for (i in seq_along(model_train_hhold_data)) {

  models[[i]] <- keras_model_sequential() %>% 
    layer_dense(units = 3000, activation = 'relu', 
      input_shape = c(ncol(x_train[[i]]))) %>% 
    layer_dropout(rate = 0.4) %>%
    layer_dense(units = 1000, activation = 'relu') %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 512, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 256, activation = 'relu') %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 50, activation = 'relu') %>%
    layer_dropout(rate = 0.05) %>%
    # softmax guarantees output between 0 and 1; pseudo probability 
    layer_dense(units = 2, activation = 'softmax') %>% 
    # 
    compile(
      # this is the metric used for the competition
      loss = 'binary_crossentropy', 
      # 
      optimizer = optimizer_rmsprop(),
      # 
      metrics = c('accuracy')
    )
}


```

arbitrary deep learning architecture  

consider using a multiscale architecture  

Other loss functions available here: https://keras.io/losses/  

```{r}

history = list() 

for (i in seq_along(models)) {

  history[[i]] <- models[[i]] %>% 
    fit(
      x = x_train[[i]], 
      y = y_train[[i]], 
      epochs = 5,  #30, 
      batch_size = 128, 
      validation_split = 0.2)
}

```


```{r}

for (i in seq_along(models)) {
  
  print(plot(history[[i]]))
  
}

```


```{r}

for (i in seq_along(models)) {
  
  temp <- models[[i]] %>% 
    evaluate(x_test[[i]], y_test[[i]])
  
  print(temp)
}


```


```{r}

for (i in seq_along(models)) {
  
  # Plot the model loss of the training data
  plot(
    history[[i]]$metrics$loss, 
    main="Model Loss", 
    xlab = "epoch", 
    ylab="loss", 
    col="blue", 
    type="l")
  
  # Plot the model loss of the test data
  lines(
    history[[i]]$metrics$val_loss, 
    col="green")
  
  # Add legend
  legend(
    "topright", 
    c("train","test"), 
    col=c("blue", "green"), 
    lty=c(1,1))
}



```


```{r}

for (i in seq_along(models)) {
  
  # Plot the model accuracy
  plot(
    history[[i]]$metrics$acc, 
    main="Model Accuracy", 
    xlab = "epoch", 
    ylab="accuracy", 
    col="blue", 
    type="l")
  
  lines(
    history[[i]]$metrics$val_acc, 
    col="green")
  
  legend(
    "bottomright", 
    c("train","test"), 
    col=c("blue", "green"), 
    lty=c(1,1))
  
}

```

### validation predictions

```{r}

test_predictions = list()

for (i in seq_along(models)) {

  test_predictions[[i]] <- models[[i]] %>%
    # make predictions
    predict_proba(x=x_test[[i]]) %>%
    # convert to df
    data.frame() %>%
    rename(not_poor=X1, poor=X2) %>%
    # add back country
    mutate(country=country[[i]]) %>%
    # add back id
    bind_cols(
      subset(model_train_hhold_data[[i]][-train_inds[[i]], ], select=c(id))) %>%
    # we only want the probability that p = 1 (not p = 0)
    select(id, country, poor)
  }

head(test_predictions)

```

### test predictions

```{r}

# need to transform test data in the same way as training data, bollocks

test_predictions = list()

for (i in seq_along(models)) {

  test_predictions[[i]] <- models[[i]] %>%
    # make predictions
    predict_proba(x=raw_test_hhold_data[[i]]) %>%
    # convert to df
    data.frame() %>%
    rename(not_poor=X1, poor=X2) %>%
    # add back country
    mutate(country=country[[i]]) %>%
    # add back id
    bind_cols(
      subset(model_train_hhold_data[[i]][-train_inds[[i]], ], select=c(id))) %>%
    # we only want the probability that p = 1 (not p = 0)
    select(id, country, poor)
  }

head(test_predictions)

```


### interpreting the model

useful resources:  
- http://www.f1-predictor.com/model-interpretability-with-shap/?utm_source=linkedin&utm_medium=post&utm_campaign=shap 
- 

insert SHAP/LIME code

## ensemble

combine multiple predictors  

useful resources:  
- https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/  


### correlation check 

check models are relatively uncorrelated  

### average

```{r}

#Predicting the probabilities
testSet$pred_rf_prob <- predict(
  object = model_rf, 
  testSet[,predictors], 
  type='prob')

testSet$pred_knn_prob <- predict(
  object = model_knn, 
  testSet[,predictors],
  type='prob')

testSet$pred_lr_prob <- predict(
  object = model_lr, 
  testSet[,predictors],
  type='prob')

# taking average of predictions
testSet$pred_avg <-(
  testSet$pred_rf_prob$Y + testSet$pred_knn_prob$Y + testSet$pred_lr_prob$Y)/3

# splitting into binary classes at 0.5
testSet$pred_avg<-as.factor(
  ifelse(
    testSet$pred_avg>0.5,
    'Y',
    'N'))

```

### weighted average

```{r}

# Taking weighted average of predictions
testSet$pred_weighted_avg <- (testSet$pred_rf_prob$Y*0.25) + 
  (testSet$pred_knn_prob$Y*0.25) +
  (testSet$pred_lr_prob$Y*0.5)

#Splitting into binary classes at 0.5
testSet$pred_weighted_avg <- as.factor(
  ifelse(
    testSet$pred_weighted_avg>0.5, 
    'Y',
    'N'))

```

### stacking  

